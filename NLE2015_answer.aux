\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Reviewer 1}{1}}
\@writefile{toc}{\contentsline {paragraph}{Comments 1-3, 5-7, 9, 11, 13-14, 31}{1}}
\@writefile{toc}{\contentsline {paragraph}{4. Page 7. 3rd equation. How is the quantity $p(X)$ calculated?}{1}}
\@writefile{toc}{\contentsline {paragraph}{8. Page 10, the penultimate paragraph, starting "The VTLN procedure..." is not clear. In the DNN that learns the warping factor, are the inputs individual feature vectors or whole utterances? The figure suggests individual feature vectors, but the sentence "Then training utterances and corresponding warping factors" suggests utterances. In the second part of this paragraph "This DNN is then used to produce the posterior probabilities of the VTLN warping factors..." needs to be clarified. Specifically you need to say that the output of this DNN is a vector, and the dimension of the vector corresponds to the number of discrete VTLN normalisation factors that are considered. This is clarified later, but it is confusing at this point.}{1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Distribution of speakers in the ChildIt corpus per grade. Children in grade 2 are approximatively 7 years old while children in grade 8 are approximatively 13 years old. }}{2}}
\newlabel{ChildItAge}{{1}{2}}
\@writefile{toc}{\contentsline {paragraph}{10.1 Page 12. The first sentence is not grammatical and needs attention.}{2}}
\@writefile{toc}{\contentsline {paragraph}{10.2 Also, there is more discussion of the "posteriors of the warping factor" and this has still not been explained, and the phrase "posteriors of the warping factor" suggests that there is only one warping factor.}{2}}
\@writefile{toc}{\contentsline {paragraph}{13. 4.1.1 and 4.1.2 say something about the transcriptions of the data that are available - word-level or phone-level?}{2}}
\@writefile{toc}{\contentsline {paragraph}{15. Page 15, 4.2.2. Please explain a little more about how the Hamming window is applied to the sequence of feature vectors. Is this normal in DNN training?}{3}}
\@writefile{toc}{\contentsline {paragraph}{16. Page 15, 1 line later, what was the rationale for choosing DCT (on the previous page you applied HLDA)}{3}}
\@writefile{toc}{\contentsline {paragraph}{17. Page 15, 2nd and 3rd paragraphs. How was the size of the network chosen? How were all of the parameters in DNN training chosen? Is performance sensitive to the precise values of these parameters?}{3}}
\@writefile{toc}{\contentsline {paragraph}{18. Page 16. 3rd line. Please explain what is meant by "... single consonants and their germinant counterparts..." I have no idea what this means.}{4}}
\@writefile{toc}{\contentsline {paragraph}{19. Page 16, 4.3. Does the HMM set used for word recognition use the same set of tied states as the previous HMM set?}{4}}
\@writefile{toc}{\contentsline {paragraph}{20. line 2. Be more precise. What exactly do you mean by saying that the DNN was trained on a different set of Gaussians? This is too imprecise.}{4}}
\@writefile{toc}{\contentsline {paragraph}{21. Page 17, 4.4. In adaptation are all of the other DNN training parameters the same as in 4.2.2}{5}}
\@writefile{toc}{\contentsline {paragraph}{22. Page 17, last line. This is the first time that the number of VTLN factors is specified, or even that it is acknowledged that only a discrete set of factors is considered and therefore it is possible to create a vector of posterior probabilities. This basic principle needs to be introduced much earlier to understand how the DNN for estimating VTLN factor posteriors works.}{5}}
\@writefile{toc}{\contentsline {paragraph}{24. Page 18, paragraph 3. In the specification of the DNN, what is the meaning of the (5021) in brackets? Does this mean that there are different numbers of tied states in different systems? If so, please explain.}{6}}
\@writefile{toc}{\contentsline {paragraph}{25. Page 18. Where does the learning rate of 0.0002 come from?}{6}}
\@writefile{toc}{\contentsline {paragraph}{26. A general point. The adapted system is close to optimal in all cases, but most of the discussion is biased towards the benefits of VTLN. For me, one of the most interesting consequences of this paper is that adaptation appears to compensate for VTL differences without any explicit VTLN. Of course, one of the problems with the "adapted system" is that model selection is not done automatically. A simple way to achieve this would be to compute the probability of an utterance for all three adapted models and then apply the highest scoring one to recognition. Why didn't you do something like this? It may be that even if this approach makes an "error" and, say, classifies a particular child as a female adult, the female adult model may give best performance on this child's data.}{7}}
\@writefile{toc}{\contentsline {paragraph}{27. Page 22. In the earlier overview of the systems, did you include the case of MFCC feature vectors augmented with VTLN warping factors obtained in the normal way?}{7}}
\@writefile{toc}{\contentsline {paragraph}{28. Page 22. Exactly how are the posterior probabilities averaged to utterance level, and why is this done?}{8}}
\@writefile{toc}{\contentsline {paragraph}{29. Page 23. Final paragraph. This is an example of what I was indicating earlier. The first line of the final paragraph is very "pro-VTLN" and ignores the fact that better performance is obtained by simple adaptation.}{8}}
\@writefile{toc}{\contentsline {paragraph}{30. Just a comment, but this seems a lot of effort to obtain a VTLN-based system that outperforms adaptation/model selection, and for children the best performance is obtained with model selection augmented with VTLN!}{8}}
\@writefile{toc}{\contentsline {paragraph}{32. Page 28. Lines 5-7. Augmenting features that have already been VTLN normalised with the posterior probabilities of the VTLN factors seems an odd thing to do. What is the motivation/justification?}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reviewer 2}{9}}
\@writefile{toc}{\contentsline {paragraph}{1. In section 4.5, the authors conduct a grid search based on a set of 25 warping factors via the standard maximum likelihood using a triphone single Gaussian system. My question is that are there any difficulties in using a more robust triphone GMM system to search the warping factors? Single Gaussian is definitely not robust enough for a reliable warping factor. The easiest argument is that at least two Gaussians are needed to model both genders. In addition, what if the warping factor is per speaker rather than per utterance?}{9}}
\@writefile{toc}{\contentsline {paragraph}{2. The speaker adaptation is achieved by retraining of an age/gender independent DNN with an age/gender specific corpus “adaptation”. How does this approach compare with the stateoftheart DNN adaptation schemes? For example, using ivectors or speaker codes as additional inputs to the DNN? What if the speakeradapted features (e.g., fMLLR) are used as the DNN inputs?}{10}}
\@writefile{toc}{\contentsline {paragraph}{3. How is system combination performed? I am not sure I understand what the authors mean by “features level” in section 5.1.4. More implementation details are appreciated. How about the recognition level combination schemes, e.g., confusion network combination? Bayesian risk decoding? etc.}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reviewer 3}{11}}
\@writefile{toc}{\contentsline {paragraph}{Comments III-V}{11}}
\@writefile{toc}{\contentsline {paragraph}{II. In the under-resourced conditions, the authors should also investigate more adaptation techniques, e.g., regularization, LIN., LON., etc. to see if they perform better. I would like to see more experimental results on more DNN based adaptation approaches.}{11}}
