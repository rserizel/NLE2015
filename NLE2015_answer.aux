\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Reviewer 1}{1}}
\@writefile{toc}{\contentsline {paragraph}{Comments 1-3, 5-7, 9, 11, 13-14, 31}{1}}
\@writefile{toc}{\contentsline {paragraph}{4. Page 7. 3rd equation. How is the quantity $p(X)$ calculated?}{1}}
\@writefile{toc}{\contentsline {paragraph}{8. Page 10, the penultimate paragraph, starting "The VTLN procedure..." is not clear. In the DNN that learns the warping factor, are the inputs individual feature vectors or whole utterances? The figure suggests individual feature vectors, but the sentence "Then training utterances and corresponding warping factors" suggests utterances. In the second part of this paragraph "This DNN is then used to produce the posterior probabilities of the VTLN warping factors..." needs to be clarified. Specifically you need to say that the outp[ut of this DNN is a vector, and the dimension of the vector corresponds to the number of discrete VTLN normalisation factors that are considered. This is clarified later, but it is confusing at this point.}{1}}
\@writefile{toc}{\contentsline {paragraph}{10.1 Page 12. The first sentence is not grammatical and needs attention.}{2}}
\@writefile{toc}{\contentsline {paragraph}{10.2 Also, there is more discussion of the "posteriors of the warping factor" and this has still not been explained, and the phrase "posteriors of the warping factor" suggests that there is only one warping factor.}{2}}
\@writefile{toc}{\contentsline {paragraph}{13. 4.1.1 and 4.1.2 say something about the transcriptions of the data that are available - word-level or phone-level?}{2}}
\@writefile{toc}{\contentsline {paragraph}{19. Page 16, 4.3. Does the HMM set used for word recognition use the same set of tied states as the previous HMM set?}{2}}
\@writefile{toc}{\contentsline {paragraph}{20. line 2. Be more precise. What exactly do you mean by saying that the DNN was trained on a different set of Gaussians? This is too imprecise.}{3}}
\@writefile{toc}{\contentsline {paragraph}{21. Page 17, 4.4. In adaptation are all of the other DNN training parameters the same as in 4.2.2}{3}}
\@writefile{toc}{\contentsline {paragraph}{22. Page 17, last line. This is the first time that the number of VTLN factors is specified, or even that it is acknowledged that only a discrete set of factors is considered and therefore it is possible to create a vector of posterior probabilities. This basic principle needs to be introduced much earlier to understand how the DNN for estimating VTLN factor posteriors works.}{3}}
\@writefile{toc}{\contentsline {paragraph}{24. Page 18, paragraph 3. In the specification of the DNN, what is the meaning of the (5021) in brackets? Does this mean that there are different numbers of tied states in different systems? If so, please explain.}{4}}
\@writefile{toc}{\contentsline {paragraph}{25. Page 18. Where does the learning rate of 0.0002 come from?}{4}}
\@writefile{toc}{\contentsline {paragraph}{32. Page 28. Lines 5-7. Augmenting features that have already been VTLN normalised with thge posteriort probabilities of the VTLN factors seems an odd thing to do. What is the motivation/justification?}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reviewer 2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reviewer 3}{5}}
\@writefile{toc}{\contentsline {paragraph}{Comments III-V}{5}}
